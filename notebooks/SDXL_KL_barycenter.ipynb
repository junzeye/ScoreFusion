{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. To reproduce images provided in our submission, the ideal runtime enviornment for this Jupyter notebook is Google Colab using an A100 GPU.\n",
    "2. You may need a Hugging Face account to download the LoRA adapters, but registeration is free.\n",
    "3. If you run into an issue with insufficient GPU memory (which might happen if you use an L4 GPU instead of A100), try decreasing the size of a batch to 1.\n",
    "4. make sure to configure the working directory of your jupyter notebook as the project root directory (NOT the `notebooks` folder) to avoid importing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and load necessary packages\n",
    "%%capture\n",
    "!pip install accelerate diffusers transformers\n",
    "!pip install -U peft\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# import custom inference & postprocessing functions we wrote\n",
    "# make sure to set your pwd as the project root directory to avoid importing errors\n",
    "from src.SDXL_inference import ScoreFusion_inference, postprocess_image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the pipeline and the two auxiliary U-Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Need to run this code first to initialize the pipeline object, so that it initiliazes cross_attention_kwargs\n",
    "%%capture\n",
    "PROFESSION = 'mathematics scientist'\n",
    "positive_prompt = f\"a photo of a {PROFESSION}, looking at the camera, ultra quality, sharp focus\"\n",
    "negative_prompt = 'cartoon, anime, 3d, painting, b&w, low quality'\n",
    "w1 = torch.tensor(0.5, device=\"cuda\", dtype=torch.float16)\n",
    "w2 = torch.tensor(0.5, device=\"cuda\", dtype=torch.float16)\n",
    "adapters=[\"white_male\", \"asian_female\"]\n",
    "\n",
    "model_dict = {\n",
    "    \"asian_female\": \"NYUAD-ComNets/Asian_Female_Profession_Model\",\n",
    "    \"white_male\": \"NYUAD-ComNets/White_Male_Profession_Model\"\n",
    "}\n",
    "models = [model_dict[name] for name in adapters]\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16).to(\"cuda\")\n",
    "for i,j in zip(models,adapters):\n",
    "    pipeline.load_lora_weights(i, weight_name=\"pytorch_lora_weights.safetensors\",adapter_name=j)\n",
    "\n",
    "pipeline.set_adapters(adapters, adapter_weights=[1.0, 0.0])\n",
    "unet1 = deepcopy(pipeline.unet).cuda()\n",
    "pipeline.set_adapters(adapters, adapter_weights=[0.0, 1.0])\n",
    "unet2 = deepcopy(pipeline.unet).cuda()\n",
    "pipeline.set_adapters(adapters, adapter_weights=[0.0, 0.0]) # unset LoRA adapters\n",
    "\n",
    "# Note: this is just for 'warmstarting' the pipeline, so disregard the images.\n",
    "_ = pipeline(prompt=positive_prompt,negative_prompt=negative_prompt,\n",
    "        num_inference_steps=2, num_images_per_prompt=1).images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample images from the ScoreFusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SIZE, NUM_BATCHES = 16, 4\n",
    "generator = torch.Generator(device=\"cuda\")\n",
    "generator.manual_seed(0) # set seed for reproducibility\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# run for 2 batches of 16-image batch\n",
    "all_latents = []\n",
    "for i in range(NUM_BATCHES):\n",
    "    images_latents = ScoreFusion_inference(pipeline=pipeline,\n",
    "        l1 = w1,\n",
    "        l2 = w2,\n",
    "        unet1 = unet1,\n",
    "        unet2 = unet2,\n",
    "        num_images_per_prompt = int(TOTAL_SIZE / NUM_BATCHES),\n",
    "        num_inference_steps = 100,\n",
    "        prompt = positive_prompt,\n",
    "        negative_prompt = negative_prompt,\n",
    "        height = 1024,\n",
    "        width = 1024,\n",
    "        generator = generator,\n",
    "        output_type = 'latent' # don't use VAE in this step; do it outisde the loop explicitly\n",
    "    ).images\n",
    "    all_latents.append(images_latents)\n",
    "    torch.cuda.empty_cache()\n",
    "batched_latents = torch.cat(all_latents, dim=0)\n",
    "\n",
    "# post-process the latents, map them back to pixel space using VAE\n",
    "torch.cuda.empty_cache()\n",
    "images = [postprocess_image(pipeline, latent.unsqueeze(0)).images[0] for latent in batched_latents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize = (16,16))\n",
    "for ax, img in zip(axs.flatten(), images[:16]):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
